\section{Section 3.5}

\subsection{Murphy 10.1}
We must write this neatly, $\vb{w}$ is a matrix and $x_i $ is a vector.
$\mu_{ik} = \text{Softmax}(w^T x_i)_k$ SInce we are only dealing wit the i'th elements here
we can drop the subscript and note that $\eta_{ij} = (w^Tx_i)_j$
\begin{align}
    \frac{\partial \mu_{ik}}{\partial \eta_{ij} } = \frac{\partial}{\partial (w^Tx_i)_j } (\frac{e^{{w^Tx_i}_k}}{\sum_l e^{{w^Tx_i}_l}}) \\
    \text{We apply the division rule to get :-} \\
    = \frac{I(k = j)e^{(w^Tx_i)_k}\sum_l e^{(w^Tx_i)_l} - e^{(w^Tx_i)_j}e^{(w^Tx_i)_k}}{(\sum_l e^{(w^Tx_i)_l})^2} \\ 
    = \mu_{ik} (I( k = j) - \mu_{ij})
\end{align}
Calculating the Hessian.


\subsection{Murphy 10.3}
LDA models linear parameter scores using MLE on the joint distribution, QDA is more general than
LDA , ie QDA contains LDA as a apecial case, so the error that is given (in terms of posterior likelihood over the training set)
for QDA will be always better than that of LDA. Similar arguments can be given for QuadLog and LinLog. QuadLog will contain LinLog and
hence the log likelihood on the dataset for QuadLog will atleast be as good as that of LinLog.
\begin{gather}
    L(\text{QuadLog}) \le L(\text{LinLog}) \\
    L(\text{QuadLog}) \le L(\text{LDA(GaussI)}) \\
    L(\text{QDA(GuassX)}) \le  ? \ge L(\text{QuadLog}) \\
\end{gather}